["RT @itsandrewgao: 2nd out of 79 teams and 300+ hackers at the @scale_AI hackathon! Shoutout to my teammates @akapoor1201 @VidithBalasa @Ale\u2026", "RT @JoeyB: Might as well win it all while we're at it", "RT @Joe_MainMixon: Part 2\ud83d\ude08", "RT @adaanahii: People are so quick to criticize vegans when we literally are only adding positivity to the world.", "Woah @__tinygrad__ way to go.  Massive achievement\n\nhttps://t.co/ZFQnJMNq2I", "reading the source code of projects using LLMs is hilarious because you can see programmers pleading with GPT 4 https://t.co/I8M0OS21jG", "My daughter asked today why it is impossible to divide by zero. \n\nThe analogy with dividing apples for people doesn\u2019t work anymore \ud83d\ude02\n\nDo you know why, without googling?", "This is exactly my view too: https://t.co/tOx9Eqprht", "I think our most crucial finding is that although humans think far ahead while speaking (especially while doing complex reasoning problems) it turns out that transformer language models.... don't seem to do that.\n\nthey just predict the next token. https://t.co/0mNUYAJ3PU", "average C bug: buried deep in a long 200 line function you forgot to unlink an element from a linked list before freeing it\n\naverage C++ bug: you wrote \u201ca = b;\u201d but unbeknownst to you \u201ca\u201d has = overloaded and the code for that makes subtle assumptions that are false in your case", "Challenge accepted! \ud83d\ude00 https://t.co/AY7rIdna0e", "TIL Noam Shazeer also built out Google\u2019s spelling correction system: https://t.co/pNgo5xUmUj", "April Fools!  \n\nThe part which isn't funny is that FFmpeg is a project written by volunteers which receives essentially no funding from VC backed Silicon Valley companies who raise tons of money off the back of FFmpeg and build their products and services using FFmpeg.", "This is similar to how I found @EricSteinb. He was just an undergrad, but he did research on his own and put a solo paper on arxiv that followed up on my work. I was impressed, so I invited him to work with me. There's a lot of talent out there with non-traditional backgrounds. https://t.co/6NLrnRybSh", "i don't want to sound cynical but do you guys think they purposefully put scenes like this in movies specifically so they can be turned into memes online https://t.co/PZrK4ZiFfk", "@LinusEkenstam I can't tell what I can trust today. Will check back tomorrow.", "\ud83c\udf89\ud83c\udf89\ud83c\udf89\n\nAlso, I'm hiring for an MLE/SWE! If you want to build LLMs with @cohere and are interested in developing challenging model evaluation settings + curating high-quality data, please reach out!\n\nICYMI: We also just opened our NYC office \ud83d\udc40 https://t.co/jRdYHWgORA", "@___frye This is literally prof eggert from ucla", "here\u2019s a fun fact: almost all modern computer systems depend on a single time zone database that gets updated when local laws change\n\nit\u2019s maintained by two people https://t.co/NBWjgot8JM", "Is anyone able to track Jia Tan\u2019s (xz backdoor person) real identity? This guy is a menace to society.", "Further reading from @lorenzofb: https://t.co/sHFZ0rTQFE https://t.co/Ih7ujqIJ9h", "Mark Zuckerberg emails Facebook execs\n\nJune 9, 2016 https://t.co/sonY5Bj74F", "xz backdoor code shipped with a good ol' LGTM https://t.co/WlVAooN7Cb", "We converted each frame to a 32x64 grid where each pixel contained the object on the frame buffer so that the LLM could \"see\". \n\nThere was a bit of prompt refinement needed here to make sure that the model could interpret the grid properly.\n\nThis video shows what the grid looks\u2026 https://t.co/yVpUCHLjRt https://t.co/gAhZirIEUT", "@krishnanrohit @AmazonKindle I stopped buying Kindle books a long time ago once I realized how weird the concept of 'ownership' is on Amazon digital", "Open-source robot arm for about $200. It uses five Dynamixel servo motors and weighs slightly over 100g (without the base). The design for a leader arm is included as well so that you can teleoperate it effectively. (Video is at 1x speed)\nhttps://t.co/2J1aS6IToF https://t.co/AtZK8C7c9z", "@carmguti @AdonAlternative I'll leave this masterpiece here. https://t.co/rXXwUIMhjw", "if the 13-year and 17-year cicadas are both emerging this summer, a double emergence that only occurs every ~200yrs, and both will concentrate in Illinois, where more than 13% of US corn is grown, should we be shorting corn futures yes or no", "@aidan_mclau Are you doing it?", "Update I replaced (X/Y) with X*(1/Y) and now it passes timing check. \n\nI learned this trick from some girl from MIT at my last job lol. Everyone is blown away with my skills. \n\nThere is no reason why it\u2019s inherently faster just synthesis tools optimize it better. https://t.co/VcduwI76iZ", "it's over https://t.co/FG2I7vpXks", "https://t.co/cLn56MVCcX", "This just dropped this morning https://t.co/gkRhORXsV9", "the @realGeorgeHotz effect https://t.co/Estmdt3gk6", "That explains \ud83d\ude05 https://t.co/yT3WWRybrn", "A lot of the value from ChatGPT comes in mundane/mindless drudgery, not deep thinking.\n\nPeople like @GaryMarcus overlook the genuine value here:\n- installing nvidia drivers\n- sorting out messed up ruby version\n- setting up custom domain name\n\nMany such cases.", "@neuralink Long-term, it is possible to shunt the signals from the brain motor cortex past the damaged part of the spine to enable people to walk again and use their arms normally", "Introducing the 01 Developer Preview.\n\nOrder or build your own today: https://t.co/ROEcj9jVPX\n\nThe 01 Light is a portable voice interface that controls your home computer. It can see your screen, use your apps, and learn new skills.\n\nThis is only the beginning for 01\u2014 the\u2026 https://t.co/plTD4NfH1b https://t.co/J5VoWlCI5i", "@cb_doge Blindsight is the next @Neuralink product after Telepathy", "today I set up vtuber software and a model all open source in like an hour. the renderer was on godot and the facial tracking was some random python script I found online. it actually works quite well and I will be hacking with it more", "&gt;cracked director of engineering recommends me this code book\n&gt;ask him if the book will help me pass leetcode FAANG interviews\n&gt;he doesn't understand\n&gt;explain him that i need to solve 4sum\n&gt;he laughs and says it's a good book\n&gt;look inside\n&gt;anatomy of a flashlight https://t.co/GP7RphsP8P", "The car was invented in 1886 but horses weren\u2019t replaced as a means for transportation until 1920\u2019s for early adopter regions and 1950\u2019s everywhere.\n\nTransformative technology doesn\u2019t appear everywhere immediately. The future is not evenly distributed and not easily distributed.", "https://t.co/OMIeGGjYtG", "Deja Vu https://t.co/QEvVkoNc3t", "it's such a shame palindrome is not a palindrome", "seriously now https://t.co/LZGP5tmxpG", "2h of debugging. Whatever you say, that's counter intuitive. https://t.co/jMtr6dFcl7", "Imagine being called 'former salesforce exec' instead of 'ML researcher with 170,000+ citations' https://t.co/duAIrIWPQu", "I was fired from OpenAI this morning.  I was the person responsible for deleting gpt35-lp-f-240108-720-joshtest-delete-or-you-will-be-fired. https://t.co/NMdMlAzmHw", "many such cases https://t.co/964SoBXnVN", "why are you booing me.  I'm right. https://t.co/VpTQPzVluX", "Bitcoin is a bounty for SHA-256", "We'll run @xai's model on @GroqInc LPUs when it's renamed to Slartibartfast.\n\nhttps://t.co/0LMu5O3JND", "\ud83d\udd25\ud83d\udd25\ud83d\udd25 https://t.co/YlgQKRg6YI", "@samschmitz Compare watt hour measurements from microprocessors whose electric connectivity or lack thereof is unknown\n\nThat's the resource. Lol\n\nOh...Linemen changed which Substation was feeding 462 of the 2,156 meters fed by this Distribution for 173.35 of the 8,760 hours last year?\n\nWe\u2026 https://t.co/MafLGdsEh7", "@LowLevelTweets oh shit forgot the sun uses internet", "my wife bought a lamp that needs our wifi password\n\nthe future blows", "No need to hire me, just open source the 7900XTX firmware+docs and remove the signature check. We'd treat it like bring up for our own chip, get builds in CI, HITL testing, fuzzing, etc...\n\nDeadline, end of the week? Otherwise I'm not spending more time thinking about this. https://t.co/x8XVy6aM4B", "We are putting resources into Intel now. multiGPU training worked out of the box, and haven't seen a lick of driver instability. The A770 card is currently slow though, we need to add XMX support ($400 bounty added) https://t.co/GPhGD4kgnT", "The AMD tinybox is on hold until we can build and run the relevant firmware on our GPUs.\n\nThe driver is still very unstable, and when it crashes or hangs we have no way of debugging it. We have no way of dumping the state of a GPU. Apparently it isn't just the MES causing these\u2026 https://t.co/CoA5bj4VYT", "## nous x replicate meetup thread \n\nRepresentation Engineering an Acid Trip \nby @voooooogel https://t.co/4wGXwO2tzi", "Bengals, OT Trent Brown agree to terms on one-year deal. (via @RapSheet) https://t.co/uPKOekunYA", "MLX Swift MNIST example got a little face-lift. \n\nTrain a LeNet on device, then draw and classify digits. Thanks to Rounak for the contribution https://t.co/ri9EPpXilu !\n\nGuide: https://t.co/hpHDhEFJQs \nRunning on my iPhone 14: https://t.co/dYuS0vzU1J", "We're announcing TacticAI: an AI assistant capable of offering insights to football experts on corner kicks. \u26bd\n\nDeveloped with @LFC, it can help teams sample alternative player setups to evaluate possible outcomes, and achieves state-of-the-art results. \ud83e\uddf5\u2026 https://t.co/Jk7NrzqXnQ https://t.co/8oVYtamGGe", "AI to read your mind is getting a LOT better.\n\nJust released: #MindEye2 is leagues ahead of previous AI systems\n\n(brain waves -&gt; CLIP space -&gt; pixels) https://t.co/JTSrJfh2RF", "I\u2019m so well prepared for this https://t.co/rUH0yzhbLs https://t.co/5i4Q1ZwFIA", "New Bengals DT Sheldon Rankins took us down memory lane today and pointed out some of the big sacks he's had against Cincinnati. (video via @Bengals) https://t.co/yxGxzzSAoI", "Researcher in deep-learning adjusting the model's architecture to beat the benchmarks. https://t.co/lFHvqb2Usp", "tinybox with 6x Intel Arc A770 is colorful! https://t.co/sM1GwIqnMn", "My boulder rolling Sisyphus rage game so far, what do you think? https://t.co/oMvo36st8q", "If you were to train GPT-4, 1.8T params model, \n\nOn A100, it will take 25k A100s and take 3-5 months.\n\nOn H100, it will take 8k GPUs and take ~3 months.\n\nOn B100, it will take 2k GPUs and take ~ 3 months.\n\n- Jenson at GTC.", "Today, we are releasing Stable Video 3D, a generative model based on Stable Video Diffusion. This new model advances the field of 3D technology, delivering greatly improved quality and multi-view.\n\nThe model is available now for commercial and non-commercial use with a Stability\u2026 https://t.co/YicNGTciax https://t.co/cc5T9RRH3L", "\ud83d\ude80 \ud835\udc02\ud835\udc28\ud835\udc21\ud835\udc1e\ud835\udc2b\ud835\udc1e \ud835\udc04\ud835\udc26\ud835\udc1b\ud835\udc1e\ud835\udc1d \ud835\udc15\ud835\udfd1 - \ud835\udc22\ud835\udc27\ud835\udc2d\ud835\udfd6 &amp; \ud835\udc1b\ud835\udc22\ud835\udc27\ud835\udc1a\ud835\udc2b\ud835\udc32 \ud835\udc12\ud835\udc2e\ud835\udc29\ud835\udc29\ud835\udc28\ud835\udc2b\ud835\udc2d\ud83d\ude80\n\nI'm excited to launch our native support for int8 &amp; binary embeddings for Cohere Embed V3.\n\nThey slash your vector DB cost 4x - 32x while keeping 95% - 100% of the search quality.\nhttps://t.co/uJBg6nyPvf https://t.co/3TVabwKm52", "Introducing Inversion: fast, reliable structured LLMs\n\nhttps://t.co/iyWUSbSbWI https://t.co/dLolpV2hYw", "We can think, even if our language abilities are destroyed by a stroke.\nLanguage is a way to express thoughts, but thoughts, memories, and reasoning exist without language.\n\nFavorite quote from this MIT piece: \"This language system seems to be distinct from regions that are\u2026 https://t.co/6TAVKGtPAP https://t.co/LX4L1MAn4Y", "We're going to talk about \"Contact Balance\" with Zack Moss https://t.co/wGgepauhNb", "Something that makes SWE a particularly feasible task for LLMs is that code is very verifiable (not in the crypto sense) and you get instant feedback\n\nIf your LLM writes code, you can instantly know if it's \"correct\" or not if it compiles.\n\nBut this isn't always the case, like\u2026 https://t.co/2mDr54plfL", "Only posting Zack Moss pass pro highlights as I watch his film https://t.co/5DitTH7fqF", "Only in #SanFrancisco does this ad make sense on a bullock at the train station.  A key rule of advertising is to know your customer.  Nailed it.  I'm thinking, \"yes, indeed, I do want to save $20M on my next H100 bill\". https://t.co/APkbMAYz85", "Vision behind the Apple Open project axlearn project is really intriguing \ud83e\uddd0 \u201cAXLearn supports the training of models with up to hundreds of billions of parameters across thousands of accelerators on a virtual global computer\u201d https://t.co/5OXUgmcifa", "Ahh, with the founder of Groq...how about a $10k wager?\n\nWe keep the machine if we succeed. You get the machine back and $10k if we don't deliver open source 500+ T/s mixtral within 8 weeks. https://t.co/VFm1wEVDgm", "At Berkeley, we drink real coffee. 6 shots of straight espresso, every morning (minimum).\n- 1x speed, teleoperated from 6,965 miles away. \n- 87ms photon-to-photon latency.\n- Unprecedented dexterity.\nHow did we do it? We lay out our unique technical approaches below and what\u2026 https://t.co/lG9tawOBVQ https://t.co/Ai3VL6aIJk", "Very nice to see Grok go open-source &amp; open-weights: https://t.co/RTlvQarPzy", "I think I speak for everyone here when I say: 314 billion parameters what the hell", "@elonmusk @xai \u2591W\u2591E\u2591I\u2591G\u2591H\u2591T\u2591S\u2591I\u2591N\u2591B\u2591I\u2591O\u2591", "Open offer.\n\nIf someone buys me a 8xH100 SXM machine, we will beat the 500 T/s of the Mixtral demo from @GroqInc on it in 8 weeks (in tinygrad!), or your machine back.", "Tenstorrent still has a long road to go, and I think aiming for LLM benchmarks and PyTorch integration is dumb. They need to be thinking on a longer timeline than this.\n\n@nvidia has moved the earth to get the interconnect bandwidths to where they are. If you told someone 10 years\u2026 https://t.co/DWIhiGyd1W", "@linaeons dingboarddddddddd", "\ud83e\udd2f crazy exchange on @modal_labs slack between @cognition_labs 's Devin &amp; modal's support team https://t.co/GZ6c5uaO5V", "EagleX 7B with only 1.7 Trillion tokens\nJust beat llama2 7B, on average in English evals\n\nBringing into open source research the best\n- multi-lingual model (beating even mistral) \n- english model at &lt;= 2T tokens (beating LLaMA)\n- all while being attention free linear transformer https://t.co/PzklFLiOOT", "CUDA-MODE 10: Build a production ready CUDA library\n\nDiscover solutions for fast prototyping, performance tuning and get a fresh take on CUDA code organization. Speaker: @morousg\n \nSat, Mar 16, 19:00 UTC\n\nhttps://t.co/VZqlR3QC76 https://t.co/ta6AcVj0Tg", "if i was on v*rcel i would have gotten a 50k bill today", "I just hit 50k req/sec on a $25/mo VM. https://t.co/ED5B8YKn6y", "The GGUF file format is a great example of the cool things that an open-source community can achieve.\n\nProps to @philpax_ and everyone else involved in the design and implementation of the format. I'm thankful and happy to see that it finds adoption in ML\n\nhttps://t.co/dRPSBUSPuN https://t.co/vERPTpRRAW", "First photo is scatter plot with a strong correlation between two variables that obviously should correlate (not to mention it is about as bivariate normal as you would see in nature). The next three are some of the replies\u2026 https://t.co/SXLf3rm5Do", "The FCC just changed the broadband internet minimum requirement from 25mbps to 100mbps https://t.co/tyEQax9rOQ", "How to write CUDA on AMD. https://t.co/CMQRMwkqgn", "I explain Objective-Driven AI, with inference by energy minimization, as opposed to Auto-Regressive prediction. https://t.co/sm3n9WPeES", "A nice example of how LLMs use statistics (a common riddle) and not understanding (what the question actually means). https://t.co/5Dtq60yxgX", "@nathanmllr funny enough though I regenerated the ChatGPT Classic (i.e. no python calculator available) answer a couple of times and in one of them it seems to have hallucinated a floating point error https://t.co/II0DlskVXC", "No.\nIf it were the case, we would have AI systems that could teach themselves to drive a car in 20 hours of practice, like any 17 year-old.\n\nBut we still don't have fully autonomous, reliable self-driving, even though we (you) have millions of hours of *labeled* training data. https://t.co/IqsAYQHfdW", "I watched @ylecun episode on @lexfridman and I think his bigger point is we should use a different architecture to actually make AI understand concepts\n\nInstead of just autocompleting word tokens like in LLMs or pixels/patches in image/video gen\n\nHe wants an AI to actually\u2026 https://t.co/OArpAOMnrh https://t.co/7scsRvFIzm"]